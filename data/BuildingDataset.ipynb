{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creation of datasets from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_FILE = '../datasets/30MDataset/relations/sessions.idomaar'\n",
    "DATASET_PROCESSED = '../Datasets/30M/raw-interactions/sessions_raw.hdf'\n",
    "DATASET_ROOT= '../Datasets/30M/'\n",
    "from itertools import (takewhile,repeat)\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "def rawincount(filename):\n",
    "    # Fastest lines counter ever!!!\n",
    "    \n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum( buf.count(b'\\n') for buf in bufgen)\n",
    "\n",
    "\n",
    "def preprocess_playlists():\n",
    "    line_count = rawincount(DATASET_FILE)\n",
    "    columns = ['user_id','session_id','created_at','item_id','item_playstart', 'item_playtime']\n",
    "    with open(DATASET_FILE, 'rt', buffering=64*1024*1024, encoding='utf8') as dataset:\n",
    "        fields = []\n",
    "        for line in tqdm(dataset,total=line_count):\n",
    "\n",
    "            line = line.split('\\t')\n",
    "            session_id = int(line[1])\n",
    "            created_at = int(line[2])\n",
    "            dict_data = json.loads(str(line[3]).split(' ')[1])\n",
    "            # process dictionary\n",
    "            user_id = dict_data['subjects'][0]['id']\n",
    "            objects = [obj['id'] for obj in dict_data['objects']]\n",
    "            objects_playtime = [obj['playtime'] for obj in dict_data['objects']]\n",
    "            objects_playstart = [obj['playstart'] for obj in dict_data['objects']]\n",
    "            for idx in range(len(objects)):\n",
    "                fields.append([user_id, session_id, created_at, objects[idx], objects[idx]+objects_playstart[idx], objects_playtime[idx]])\n",
    "            \n",
    "        df = pd.DataFrame(fields, columns=columns)\n",
    "        df.to_hdf(DATASET_PROCESSED, key='raw')\n",
    "    del df\n",
    "    del fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_FILE_TIANCHI = '../datasets/Repeat Buyers Prediction-Challenge the Baseline/data_format1/user_log_format1.csv'\n",
    "DATASET_FILE_TIANCHI_PRE = '../datasets/Repeat Buyers Prediction-Challenge the Baseline/data_format1/user_log_format1_pre.csv'\n",
    "DATASET_PROCESSED_TIANCHI = '../Datasets/Tianchi/raw-interactions/sessions_raw.hdf'\n",
    "DATASET_ROOT_TIANCHI= '../Datasets/Tianchi/'\n",
    "from itertools import (takewhile,repeat)\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "def rawincount(filename):\n",
    "    from itertools import takewhile\n",
    "    # Fastest lines counter ever!!!\n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum( buf.count(b'\\n') for buf in bufgen)\n",
    "def select_colums_from_source():\n",
    "    source = pd.read_csv(DATASET_FILE_TIANCHI,usecols=['user_id','item_id','time_stamp','action_type'])\n",
    "    source.sort_values(['user_id','time_stamp'],inplace=True)\n",
    "    source.to_csv(DATASET_FILE_TIANCHI_PRE,index=False)\n",
    "    del source\n",
    "def preprocess_log():\n",
    "    \n",
    "    line_count = rawincount(DATASET_FILE_TIANCHI_PRE)\n",
    "    columns = ['user_id','item_id','interaction_type','created_at']\n",
    "    with open(DATASET_FILE_TIANCHI_PRE, 'rt', buffering=64*1024*1024*8, encoding='utf8') as dataset:\n",
    "        fields = []\n",
    "        dataset.__next__()\n",
    "        line_number= 0\n",
    "        date_ = '----'\n",
    "        offset=0\n",
    "        for line in tqdm(dataset,total=line_count):\n",
    "            line_number +=1\n",
    "            line = line.split(',')\n",
    "            user_id = line[0]\n",
    "            timestamp = line[2]\n",
    "            item_id = line[1]\n",
    "            action_type = line[3].strip()\n",
    "        \n",
    "            if date_ != timestamp:\n",
    "                offset = 0\n",
    "                date_ = timestamp\n",
    "                ts = datetime.strptime(timestamp, '%m%d').timestamp()\n",
    "            else:\n",
    "                offset += 1\n",
    "            \n",
    "            new_line = [user_id, item_id, action_type,ts+offset]\n",
    "            fields.append(new_line)\n",
    "            if line_number % 1e7 == 0:\n",
    "                df = pd.DataFrame(fields, columns=columns)\n",
    "                df.to_hdf(DATASET_PROCESSED_TIANCHI, key='raw',mode='a')\n",
    "                fields=[]\n",
    "        #saving last slice\n",
    "        df = pd.DataFrame(fields, columns=columns)\n",
    "        df.to_hdf(DATASET_PROCESSED_TIANCHI, key='raw',mode='a')\n",
    "    del df\n",
    "    del fields\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Logs from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 54925330/54925331 [03:27<00:00, 264787.87it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_playlists()\n",
    "preprocess_log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Filtering data according to baseline critheria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inspired on the original data filtering from HGRU4REC\n",
    "import subprocess\n",
    "import numpy as np\n",
    "def make_sessions(data, session_th=30 * 60, is_ordered=False, user_key='user_id', item_key='item_id', time_key='ts'):\n",
    "    \"\"\"Assigns session ids to the events in data without grouping keys\"\"\"\n",
    "    if not is_ordered:\n",
    "        # sort data by user and time\n",
    "        data.sort_values(by=[user_key, time_key], ascending=True, inplace=True)\n",
    "    # compute the time difference between queries\n",
    "    tdiff = np.diff(data[time_key].values)\n",
    "    # check which of them are bigger then session_th\n",
    "    split_session = tdiff > session_th\n",
    "    split_session = np.r_[True, split_session]\n",
    "    # check when the user chenges is data\n",
    "    new_user = data['user_id'].values[1:] != data['user_id'].values[:-1]\n",
    "    new_user = np.r_[True, new_user]\n",
    "    # a new sessions stars when at least one of the two conditions is verified\n",
    "    new_session = np.logical_or(new_user, split_session)\n",
    "    # compute the session ids\n",
    "    session_ids = np.cumsum(new_session)\n",
    "    data['session_id'] = session_ids\n",
    "    return data\n",
    "\n",
    "    \n",
    "def last_session_out_split(data,\n",
    "                           user_key='user_id',\n",
    "                           item_key='item_id',\n",
    "                           session_key='session_id',\n",
    "                           time_key='ts',\n",
    "                           clean_test=True,\n",
    "                           min_session_length=2):\n",
    "    \"\"\"\n",
    "    last-session-out split\n",
    "    assign the last session of every user to the test set and the remaining ones to the training set\n",
    "    \"\"\"\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    return train, test\n",
    "\n",
    "def last_session_out_split_policy_train(data,\n",
    "                           user_key='user_id',\n",
    "                           item_key='item_id',\n",
    "                           session_key='session_id',\n",
    "                           time_key='ts',\n",
    "                           clean_test=True,\n",
    "                           min_session_length=2):\n",
    "    \"\"\"\n",
    "    last-session-out split\n",
    "    assign the last session of every user to the test set and the remaining ones to the training set\n",
    "    \"\"\"\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    return train_env, train_policy, test\n",
    "\n",
    "def last_n_days_out_split(data, n=1,\n",
    "                          user_key='user_id',\n",
    "                          item_key='item_id',\n",
    "                          session_key='session_id',\n",
    "                          time_key='ts',\n",
    "                          clean_test=True,\n",
    "                          min_session_length=2):\n",
    "    \"\"\"\n",
    "    last n-days out split\n",
    "    assign the sessions in the last n days to the test set and remaining to the training one\n",
    "    \"\"\"\n",
    "    DAY = 24 * 60 * 60\n",
    "    data.sort_values(by=[user_key, time_key], inplace=True)\n",
    "    sessions_start = data.groupby(session_key)[time_key].agg('min')\n",
    "    end_time = data[time_key].max()\n",
    "    test_start = end_time - n * DAY\n",
    "    train = data[data.session_id.isin(sessions_start[sessions_start < test_start].index)].copy()\n",
    "    test = data[data.session_id.isin(sessions_start[sessions_start >= test_start].index)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    return train, test\n",
    "\n",
    "def ratio_session_split(data, \n",
    "                        keep_ratio=0.5,\n",
    "                        user_key='user_id',\n",
    "                        item_key='item_id',\n",
    "                        session_key='session_id',\n",
    "                        time_key='ts',\n",
    "                        clean_test=True,\n",
    "                        min_session_length=2):\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    \n",
    "    return train_1 ,test_1, train_2, test_2\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(interactions, mode=['raw','full'],dataset_dir=''):\n",
    "    print('Mode: '+'-'.join(mode))\n",
    "    print('Filtering data')\n",
    "    \n",
    "    # drop duplicate interactions within the same session\n",
    "    \n",
    "\n",
    "    if mode[0] == 'dense':\n",
    "        interactions.drop_duplicates(subset=['item_id', 'session_id'], keep='first', inplace=True)\n",
    "    # keep items with >=20 interactions\n",
    "    item_pop = interactions.item_id.value_counts()\n",
    "    good_items = item_pop[item_pop >= 20].index\n",
    "    inter_dense = interactions[interactions.item_id.isin(good_items)]\n",
    "    # remove sessions with length < 3\n",
    "    session_length = inter_dense.session_id.value_counts()\n",
    "    good_sessions = session_length[session_length >= 3].index\n",
    "    inter_dense = inter_dense[inter_dense.session_id.isin(good_sessions)]\n",
    "    # let's keep only returning users (with >= 5 sessions) and remove overly active ones (>=200 sessions)\n",
    "    sess_per_user = inter_dense.groupby('user_id')['session_id'].nunique()\n",
    "    good_users = sess_per_user[(sess_per_user >= 5) & (sess_per_user < 200)].index\n",
    "    #selection 1000 users for small dataset\n",
    "    if mode[1] == 'small':\n",
    "        good_users = good_users[:1000]\n",
    "    inter_dense = inter_dense[inter_dense.user_id.isin(good_users)]\n",
    "\n",
    "\n",
    "\n",
    "    print('Filtered data:')\n",
    "    print('Num items: {}'.format(inter_dense.item_id.nunique()))\n",
    "    print('Num users: {}'.format(inter_dense.user_id.nunique()))\n",
    "    print('Num sessions: {}'.format(inter_dense.session_id.nunique()))\n",
    "\n",
    "    print('Partitioning data')\n",
    "    # last-session-out partitioning\n",
    "    train_full_sessions, test_sessions = last_session_out_split(inter_dense,\n",
    "                                                                user_key='user_id',\n",
    "                                                                item_key='item_id',\n",
    "                                                                session_key='session_id',\n",
    "                                                                time_key='created_at',\n",
    "                                                                clean_test=True)\n",
    "    train_valid_sessions, valid_sessions = last_session_out_split(train_full_sessions,\n",
    "                                                                  user_key='user_id',\n",
    "                                                                  item_key='item_id',\n",
    "                                                                  session_key='session_id',\n",
    "                                                                  time_key='created_at',\n",
    "                                                                  clean_test=True)\n",
    "\n",
    "    print('Write to disk')\n",
    "    # write to disk\n",
    "    mode_folder = dataset_dir+'-'.join(mode)\n",
    "    subprocess.call(['mkdir', '-p', mode_folder+'/last-session-out'])\n",
    "    train_full_sessions.to_hdf(mode_folder+'/last-session-out/sessions.hdf','train')\n",
    "    test_sessions.to_hdf(mode_folder+'/last-session-out/sessions.hdf','test')\n",
    "    train_valid_sessions.to_hdf(mode_folder+'/last-session-out/sessions.hdf', 'valid_train')\n",
    "    valid_sessions.to_hdf(mode_folder+'/last-session-out/sessions.hdf','valid_test')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating 30M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: raw-full\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 232399\n",
      "Num users: 37667\n",
      "Num sessions: 1213283\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: raw-small\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 103327\n",
      "Num users: 1000\n",
      "Num sessions: 32135\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: dense-full\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 193726\n",
      "Num users: 36832\n",
      "Num sessions: 1121096\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: dense-small\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 97595\n",
      "Num users: 1000\n",
      "Num sessions: 30163\n",
      "Partitioning data\n",
      "Write to disk\n"
     ]
    }
   ],
   "source": [
    "#reading data from origial source\n",
    "#preprocess_playlists()\n",
    "# Reading raw data\n",
    "\n",
    "interactions = pd.read_hdf(DATASET_PROCESSED, key='raw')\n",
    "create_dataset(interactions, mode=['raw','full'],dataset_dir=DATASET_ROOT)\n",
    "create_dataset(interactions, mode=['raw','small'],dataset_dir=DATASET_ROOT)\n",
    "create_dataset(interactions, mode=['dense','full'],dataset_dir=DATASET_ROOT)\n",
    "create_dataset(interactions, mode=['dense','small'],dataset_dir=DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tianchi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing negative events\n",
      "Building sessions\n",
      "Mode: raw-full\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 49805\n",
      "Num users: 19992\n",
      "Num sessions: 238559\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: raw-small\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 27670\n",
      "Num users: 1000\n",
      "Num sessions: 11995\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: dense-full\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 31497\n",
      "Num users: 12799\n",
      "Num sessions: 137429\n",
      "Partitioning data\n",
      "Write to disk\n",
      "Mode: dense-small\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 23367\n",
      "Num users: 1000\n",
      "Num sessions: 10928\n",
      "Partitioning data\n",
      "Write to disk\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_hdf(DATASET_PROCESSED_TIANCHI, key='raw')\n",
    "print('Removing negative events')\n",
    "negative_events = [4]\n",
    "interactions = interactions[~interactions.interaction_type.isin(negative_events)].copy()\n",
    "print('Building sessions')\n",
    "#partition interactions into sessions with 30-minutes idle time\n",
    "interactions = make_sessions(interactions, session_th=60 * 60, time_key='created_at', is_ordered=False)\n",
    "\n",
    "\n",
    "create_dataset(interactions, mode=['raw','full'],dataset_dir=DATASET_ROOT_TIANCHI)\n",
    "create_dataset(interactions, mode=['raw','small'],dataset_dir=DATASET_ROOT_TIANCHI)\n",
    "create_dataset(interactions, mode=['dense','full'],dataset_dir=DATASET_ROOT_TIANCHI)\n",
    "create_dataset(interactions, mode=['dense','small'],dataset_dir=DATASET_ROOT_TIANCHI)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
